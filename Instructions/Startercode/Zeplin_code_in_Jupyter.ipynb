{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Load in user_data.csv from S3 into a DataFrame\n",
    "url = \"https://s3.amazonaws.com/globalwarmingprj3/Countries_Lats.csv\"\n",
    "\n",
    "#  https://globalwarmingprj3.s3.amazonaws.com/Countries_Lats.csv\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "countries_data_df = spark.read.option('header', 'true').csv(SparkFiles.get(\"Countries_Lats.csv\"), inferSchema=True, sep=',')\n",
    "countries_data_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "url = \"https://s3.amazonaws.com/globalwarmingprj3/GlobalLandTemperaturesByCountry-cleaned.csv\"\n",
    "\n",
    "#  https://globalwarmingprj3.s3.amazonaws.com/GlobalLandTemperaturesByCountry-cleaned.csv\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "land_temp_data_df = spark.read.option('header', 'true').csv(SparkFiles.get(\"GlobalLandTemperaturesByCountry-cleaned.csv\"), inferSchema=True, sep=',')\n",
    "land_temp_data_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "url = \"https://s3.amazonaws.com/globalwarmingprj3/greenhouse-gas-emissions-by-sector-cleaned.csv\"\n",
    "\n",
    "#  https://globalwarmingprj3.s3.amazonaws.com/greenhouse-gas-emissions-by-sector-cleaned.csv\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "green_house_data_df = spark.read.option('header', 'true').csv(SparkFiles.get(\"greenhouse-gas-emissions-by-sector-cleaned.csv\"), inferSchema=True, sep=',')\n",
    "green_house_data_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "# Configure settings for RDS\n",
    "mode = \"append\"\n",
    "jdbc_url=\"jdbc:postgresql://global-warming-rds.cvy0ocn3ij6n.us-east-2.rds.amazonaws.com:5432/global_warming_db\"\n",
    "config = {\"user\":\"root\", \n",
    "          \"password\": \"globalwarming\", \n",
    "          \"driver\":\"org.postgresql.Driver\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create dataframe to match countries_lat_long table \n",
    "\n",
    "clean_countries_df = countries_data_df.select(col(\"code\").alias(\"country_id\"),col(\"latitude\"),col(\"longitude\"),col(\"country\").alias(\"country_name\"))\n",
    "\n",
    "\n",
    "#clean_countries_df = countries_data_df.select([\"country_id\", \"latitude\", \"longitude\", \"country_name\"])\n",
    "clean_countries_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_countries_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8f2f62b3a261>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Append DataFrame to active_user table in RDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclean_countries_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mjdbc_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'country_lat_long'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'append'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproperties\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_countries_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Append DataFrame to active_user table in RDS\n",
    "clean_countries_df.write.jdbc( jdbc_url,'country_lat_long',mode='append',properties=config);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "#converting spark dataframe to pandas dataframe\n",
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window  \n",
    "\n",
    "land_temp_data_id_df = land_temp_data_df.withColumn('Id', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "land_temp_data_id_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "\n",
    "#drop null values\n",
    "drop_na_land_temp_df = land_temp_data_id_df.dropna()\n",
    "drop_na_land_temp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create dataframe to match global_land_temps table \n",
    "clean_land_temp_df = drop_na_land_temp_df.select(col(\"Id\").alias(\"Lt_id\"),col(\"dt\").alias(\"Lt_date\"),col(\"AverageTemperature\").alias(\"Avg_temp\"),col(\"AverageTemperatureUncertainty\").alias(\"Avg_temp_un\"),col(\"code\").alias(\"Country_id\"))\n",
    "\n",
    "clean_land_temp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "\n",
    "#converting the date column from string to date format to insert into postgreSQL\n",
    "#the date is converted to default spark format i.e \"yyyy-mm-dd\"\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date\n",
    "\n",
    "type_cnvrt_land_temp_df = clean_land_temp_df.withColumn('Lt_date', \n",
    "                   to_date(unix_timestamp(col('Lt_date'), 'dd/mm/yyyy').cast(\"timestamp\")))\n",
    "\n",
    "type_cnvrt_land_temp_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "\n",
    "# Append DataFrame to global_land_temps table in RDS\n",
    "type_cnvrt_land_temp_df.write.jdbc(jdbc_url,'global_land_temps',mode='append',properties=config);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import desc, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window  \n",
    "\n",
    "green_house_data_id_df = green_house_data_df.withColumn('Id', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "green_house_data_id_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "\n",
    "#drop null values\n",
    "drop_na_green_house_df = green_house_data_id_df.dropna()\n",
    "drop_na_green_house_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create dataframe to match green_house_gases table \n",
    "clean_green_house_df = drop_na_green_house_df.select(col(\"Id\").alias(\"Gh_id\"),col(\"Country\").alias(\"Country\"),col(\"Code\").alias(\"Country_id\"),col(\"Year\").alias(\"Year\"),col(\"Other sources (tonnes)\").alias(\"Other_source\"),col(\"Waste (tonnes)\").alias(\"Waste\"),col(\"Industry (tonnes)\").alias(\"Industry\"),col(\"Residential & commercial (tonnes)\").alias(\"Residential\"),col(\"Transport (tonnes)\").alias(\"Transport\"),col(\"Agriculture (tonnes)\").alias(\"Agriculture\"),col(\"Forestry (tonnes)\").alias(\"Forestry\"),col(\"Land use sources (tonnes)\").alias(\"Land_use\"),col(\"Energy (tonnes)\").alias(\"Energy\"))\n",
    "\n",
    "#clean_land_temp_df.show()\n",
    "clean_green_house_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "\n",
    "# Append DataFrame to green_house_gases table in RDS\n",
    "clean_green_house_df.write.jdbc(jdbc_url,'green_house_gases',mode='append',properties=config);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "\n",
    "green_house_from_db = sqlContext.read.jdbc(url=jdbc_url, table=\"green_house_gases\", properties=config)\n",
    "green_house_from_db.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "green_house_pandas_df = green_house_from_db.select(\"*\").toPandas()\n",
    "green_house_pandas_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "green_house_pandas_it = green_house_pandas_df.loc[green_house_pandas_df[\"country\"]==\"Italy\",:]\n",
    "\n",
    "green_house_pandas_x = green_house_pandas_it.loc[green_house_pandas_it[\"year\"]>=1910,:]\n",
    "green_house_pandas_x\n",
    "\n",
    "green_house_pandas_y = green_house_pandas_it[\"waste\"]\n",
    "green_house_pandas_y\n",
    "\n",
    "green_house_chart = green_house_pandas_y.plot(kind=\"bar\")\n",
    "plt.show()\n",
    "#plt.bar(green_house_pandas_x, green_house_pandas_y, color='r', alpha=0.5, align=\"center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "\n",
    "global_land_from_db = sqlContext.read.jdbc(url=jdbc_url, table=\"global_land_temps\", properties=config)\n",
    "global_land_from_db.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "\n",
    "global_land_pandas = global_land_from_db.select(\"*\").toPandas()\n",
    "#global_land_pandas\n",
    "\n",
    "#change date column to date format to insert into postgres\n",
    "global_land_pandas['lt_date'] = pd.to_datetime(global_land_pandas['lt_date'])\n",
    "#global_land_pandas.info()\n",
    "\n",
    "#Extracting the year from the date\n",
    "global_land_pandas['year'] = pd.DatetimeIndex(global_land_pandas['lt_date']).year\n",
    "global_land_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "\n",
    "years = np.unique(global_land_pandas['year'])\n",
    "\n",
    "mean_temp_world = []\n",
    "mean_temp_world_uncertainty = []\n",
    "\n",
    "for year in years:\n",
    "    mean_temp_world.append(global_land_pandas[global_land_pandas['year'] == year]['avg_temp'].mean())\n",
    "    mean_temp_world_uncertainty.append(global_land_pandas[global_land_pandas['year'] == year]['avg_temp_un'].mean())\n",
    "\n",
    "trace0 = go.Scatter(\n",
    "    x = years, \n",
    "    y = np.array(mean_temp_world) + np.array(mean_temp_world_uncertainty),\n",
    "    fill= None,\n",
    "    mode='lines',\n",
    "    name='Uncertainty top',\n",
    "    line=dict(\n",
    "        color='rgb(0, 255, 255)',\n",
    "    )\n",
    ")\n",
    "trace1 = go.Scatter(\n",
    "    x = years, \n",
    "    y = np.array(mean_temp_world) - np.array(mean_temp_world_uncertainty),\n",
    "    fill='tonexty',\n",
    "    mode='lines',\n",
    "    name='Uncertainty bot',\n",
    "    line=dict(\n",
    "        color='rgb(0, 255, 255)',\n",
    "    )\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = years, \n",
    "    y = mean_temp_world,\n",
    "    name='Average Temperature',\n",
    "    line=dict(\n",
    "        color='rgb(199, 121, 093)',\n",
    "    )\n",
    ")\n",
    "data = [trace0, trace1, trace2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title='year'),\n",
    "    yaxis=dict(title='Average Temperature, Â°C'),\n",
    "    title='Average land temperature in world',\n",
    "    showlegend = False)\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "x = years\n",
    "y = mean_temp_world\n",
    "\n",
    "#plot data\n",
    "#plt.plot(x, y, marker=\"o\", color=\"red\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "show(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "#green_house_pandas_df.head(10)\n",
    "mean_vals = []\n",
    "mean_vals = green_house_pandas_df.mean()\n",
    "\n",
    "mean_other = green_house_pandas_df['other_source'].mean()\n",
    "mean_waste = green_house_pandas_df['waste'].mean()\n",
    "mean_industry = green_house_pandas_df['industry'].mean()\n",
    "mean_residenial = green_house_pandas_df['residential'].mean()\n",
    "mean_transport = green_house_pandas_df['transport'].mean()\n",
    "mean_agriculture = green_house_pandas_df['agriculture'].mean()\n",
    "mean_forestry = green_house_pandas_df['forestry'].mean()\n",
    "mean_land_use = green_house_pandas_df['land_use'].mean()\n",
    "mean_energy = green_house_pandas_df['energy'].mean()\n",
    "\n",
    "mean_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "pies = [\"Other\",\"Waste\", \"Industry\", \"Residential\", \"Transport\", \"Agriculture\", \"Forestry\", \"Land Use\", \"Energy\"]\n",
    "pie_votes = [mean_other,mean_waste,mean_industry,mean_residenial,mean_transport,mean_agriculture,mean_forestry,mean_land_use,mean_energy]\n",
    "colors = [\"yellow\",\"green\",\"lightblue\",\"orange\",\"red\",\"purple\",\"pink\",\"yellowgreen\",\"lightskyblue\"]\n",
    "explode = (0.1,0,0,0,0,0,0,0,0)\n",
    "\n",
    "# Tell matplotlib to create a pie chart based upon the above data\n",
    "plt.pie(pie_votes, explode=explode, labels=pies, colors=colors,autopct=\"%1.1f%%\")\n",
    "# Create axes which are equal so we have a perfect circle\n",
    "plt.axis(\"equal\")\n",
    "# Save an image of our chart and print the final product to the screen\n",
    "#plt.savefig(\"../Images/PyPies.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pyspark` not found.\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "\n",
    "green_house_pandas_us = green_house_pandas_df.loc[green_house_pandas_df[\"country_id\"] == \"US\",:]\n",
    "\n",
    "mean_other = green_house_pandas_us['other_source'].mean()\n",
    "mean_waste = green_house_pandas_us['waste'].mean()\n",
    "mean_industry = green_house_pandas_us['industry'].mean()\n",
    "mean_residenial = green_house_pandas_us['residential'].mean()\n",
    "mean_transport = green_house_pandas_us['transport'].mean()\n",
    "mean_agriculture = green_house_pandas_us['agriculture'].mean()\n",
    "mean_forestry = green_house_pandas_us['forestry'].mean()\n",
    "mean_land_use = green_house_pandas_us['land_use'].mean()\n",
    "mean_energy = green_house_pandas_us['energy'].mean()\n",
    "\n",
    "pies = [\"Other\",\"Waste\", \"Industry\", \"Residential\", \"Transport\", \"Agriculture\", \"Forestry\", \"Land Use\", \"Energy\"]\n",
    "pie_votes = [mean_other,mean_waste,mean_industry,mean_residenial,mean_transport,mean_agriculture,mean_forestry,mean_land_use,mean_energy]\n",
    "colors = [\"yellow\",\"green\",\"lightblue\",\"orange\",\"red\",\"purple\",\"pink\",\"yellowgreen\",\"lightskyblue\"]\n",
    "explode = (0.1,0,0,0,0,0,0,0,0)\n",
    "\n",
    "# Tell matplotlib to create a pie chart based upon the above data\n",
    "plt.pie(pie_votes, explode=explode, labels=pies, colors=colors,autopct=\"%1.1f%%\")\n",
    "# Create axes which are equal so we have a perfect circle\n",
    "plt.axis(\"equal\")\n",
    "# Save an image of our chart and print the final product to the screen\n",
    "#plt.savefig(\"../Images/PyPies.png\")\n",
    "plt.show()\n",
    "\n",
    "mean_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-51381d149b74>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-29-51381d149b74>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    df =\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%pyspark\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "\n",
    "colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "\n",
    "opt = []\n",
    "opts = []\n",
    "for i in range(0, len(colors)):\n",
    "    opt = dict(\n",
    "        target = df['continent'][[i]].unique(), value = dict(marker = dict(color = colors[i]))\n",
    "    )\n",
    "    opts.append(opt)\n",
    "\n",
    "data = [dict(\n",
    "  type = 'scatter',\n",
    "  mode = 'markers',\n",
    "  x = df['lifeExp'],\n",
    "  y = df['gdpPercap'],\n",
    "  text = df['continent'],\n",
    "  hoverinfo = 'text',\n",
    "  opacity = 0.8,\n",
    "  marker = dict(\n",
    "      size = df['pop'],\n",
    "      sizemode = 'area',\n",
    "      sizeref = 200000\n",
    "  ),\n",
    "  transforms = [\n",
    "      dict(\n",
    "        type = 'filter',\n",
    "        target = df['year'],\n",
    "        orientation = '=',\n",
    "        value = 2007\n",
    "      ),\n",
    "      dict(\n",
    "        type = 'groupby',\n",
    "        groups = df['continent'],\n",
    "        styles = opts\n",
    "    )]\n",
    ")]\n",
    "\n",
    "layout = dict(\n",
    "    yaxis = dict(\n",
    "        type = 'log'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_dict = dict(data=data, layout=layout)\n",
    "pio.show(fig_dict, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
